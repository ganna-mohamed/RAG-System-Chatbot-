{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001d6716-2a22-47e1-9fa0-b3215ecaddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"faiss-cpu install failed; please install a compatible faiss for your platform\"\n"
     ]
    }
   ],
   "source": [
    "#  Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install streamlit==1.25.0 PyPDF2==3.0.1 langchain==0.0.300 sentence-transformers==2.2.2 transformers==4.35.0 torch --quiet\n",
    "!pip install faiss-cpu --quiet || echo \"faiss-cpu install failed; please install a compatible faiss for your platform\"\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c67604-9bbb-4495-a355-f935037de653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n",
      "Torch available: 2.8.0+cpu CUDA: False\n",
      "transformers version: unknown\n",
      "WARNING:tensorflow:From C:\\Users\\C-LAB\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "sentence_transformers version: unknown\n",
      "langchain version: unknown\n",
      "faiss version: unknown\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” Quick environment check\n",
    "import sys, pkgutil\n",
    "import torch\n",
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "print(\"Torch available:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "# show few installed package versions (sanity)\n",
    "import importlib\n",
    "for pkg in (\"transformers\", \"sentence_transformers\", \"langchain\", \"faiss\"):\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        print(pkg, \"version:\", getattr(m, \"_version_\", \"unknown\"))\n",
    "    except Exception as e:\n",
    "        print(pkg, \"not importable:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a5e60e-852b-43f6-9d8f-ddfccdb2a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers==4.44.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55051d4b-93ae-4420-9893-0acbf5b5a231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ee5038-dda9-4d96-813a-9b32b84e55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence-transformers/all-MiniLM-L6-v2 (may take a moment)...\n",
      "Embeddings model loaded.\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b385553e274921b1dc8da5daa31744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  49%|####8     | 482M/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C-LAB\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\C-LAB\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b061cc717c6d4e0a97be87e13c246ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece55ff63a4241c5a4d972b9ab98b9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558f691f673444e491c52d500c32b5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90ebc3394bb4ca2b6dd4ff49c868b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e704d8136724295b2e611cf024a771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator pipeline loaded. Example:\n",
      "Streamlit PDF chatbot that answers questions about PDF content. Streamlit PDF chatbot that answers questions about PDF content. Streamlit PDF chatbot that answers questions about PDF content. Streamlit PDF chatbot that answers questions about PDF content. Streamlit PDF chatbot that answers questions about PDF content. Streamlit PDF chatbot that answers questions about PDF content.\n"
     ]
    }
   ],
   "source": [
    "#  Quick model loading (embeddings + generator)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# embeddings test\n",
    "print(\"Loading sentence-transformers/all-MiniLM-L6-v2 (may take a moment)...\")\n",
    "s = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Embeddings model loaded.\")\n",
    "\n",
    "# generator test\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using device:\", (\"cuda\" if device==0 else \"cpu\"))\n",
    "gen = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device=device if device>=0 else -1,\n",
    "    max_length=200\n",
    ")\n",
    "print(\"Generator pipeline loaded. Example:\")\n",
    "print(gen(\"Summarize: Streamlit PDF chatbot that answers questions about PDF content.\", max_length=80)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130aa588-b5d0-4eef-9a51-b866c4543e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
